機器學習三步驟
1. 定義模型
2. 評估模型
	1.線性回歸模型我們可以使用均方差 (mean square error) 來衡量
	2.目標函數 (Objective function) 也可稱作損失函數 (Loss function)，來衡量模型的好壞
3. 訓練模型

(Over-fitting)
1.過擬合代表模型可能學習到資料中的噪音，導致在實際應用時預測失準

監督式學習 : 
1. 前處理
	1. 資料讀取
	2. 格式調整
	3. 型態特徵調整
		1. 數值型
			1. 去離群值
			2. 去偏態
			3. 特徵縮放
			4. 填補缺值
		2. 類別型
			1.填補缺值
			2.基本處理
				1.Label Encoding (適合非監督)
				2.one Hard Encoding (適合深度)
			3. 均值編碼
				1.當類別特徵與目標明顯相關時，該考慮採用均值編碼
				2.均值編碼最大的問題，在於相當容易 Overfitting 
				3.平滑化的方式能修正均值編碼容易 Overfitting 的問題，但效果有限
		3. 時間型
			1.填補缺值
			2.時間型特徵最常用的是特徵分解 - 拆解成年/月/日/時/分/秒的分類值
			3.週期循環特徵是將時間"循環"特性改成特徵方式，設計關鍵在於首尾相接, 因此我們需要使用 sin /cos 等週期函數轉換
				1.常見的週期循環特徵有 - 年週期(季節) / 周周期(例假日) / 日週期(日夜與生活作息), 要注意的是最高與最低點的設置
				2. [日期處理小結]https://wklken.me/posts/2015/03/03/python-base-datetime.html
	4. 特徵組合 / 特徵篩選
		1. 增加特徵 
			1. 特徵組合
			2. 群聚編碼
		2.減少特徵
		 	1.特徵選擇
				1.過濾法
				2.包裝法
				3.嵌入法
	5. 特徵評估
2. 探索數據分析
	1. 相關係數
	2. 離散化
	3. 常用圖形
	4. 繪圖排版
3. 模型選擇
	1.驗證集
		1. 訓練/測試拆分
	2.預測類型
		1.回歸與分類定義
			1.回歸 : 連續數
			2.分類 : bool
	3.評估指標
		**重點
			1.回歸問題可以透過 R-square 很快了解預測的準確程度
			2.分類問題若為二分類 (binary classification)，通常使用 AUC 評估。但如果有特別希望哪一類別不要分錯，則可使用 F1-Score
		1.設定各項指標來評估模型預測的準確性
			1.Accuracy = 正確分類樣本數/總樣本數
		2. 評估回歸 (差距)
			1.MAE
			2.MSE
			3.R-square
		3. 評估分類 (正確性)
			1.AUC
				1.需要定義閾值 (threshold)來決定分類的類別
			2.F1-Score (Precision, Recall 的調和平均數)
				1.Precision: 模型判定瑕疵，樣本確實為瑕疵的比例
				2.Recall:  模型判定的瑕疵，佔樣本所有瑕疵的比例
			3.混淆矩陣 (Confusion Matrix)
	4.基礎模型
		1.線性回歸 : 簡單常見的線性模型
		2.羅吉斯回歸 : 是分類模型，將線性回歸的結果，加上 Sigmoid 函數，將預測值限制在 0 ~ 1 之間
		3.LASSO(套索算法
			1. 為 Linear Regression 加上 L1 
		3.Ridge(領回歸
			2. 為 Linear Regression 加上 L2
		簡單來說，LASSO 與 Ridge 就是回歸模型加上不同的正則化函數
	5.樹狀模型
		1.決策數
			1.gini index 、 entropy
		2.隨機森林
			1.決策樹非常容易 Over-fitting ，所以有隨機森林
			2.透過投票或是加權的方式得到最終結果
			3.實務上不太會有隨機森林比決策樹差的情形
			4.隨機森林使用的集成方法稱為 Bagging (Bootstrap aggregating)，用抽樣的資料與 features 生成每一棵樹，最後再取平均
		3.梯度提升機
			1.Boosting 一種集成方法，希望能夠由後面生成的樹，來修正前面樹學不好的地方
		4.Gradient boosting 與 Random Forest 的差別
			1.Random Forest : 每棵樹都獨立
			2.每棵樹都是為了修正前一棵的錯誤，因此每一顆皆有關連性
		5.Bagging 與 Boosting 的差別
			1.Bagging 是透過抽樣 (sampling) 的方式來生成每一棵樹
			2.Boosting 是透過序列 (additive) 的方式來生成每一顆樹
4. 參數調整 (資料清理與特徵工程才是最有效的)
	1.超參數設置 (建議先使用預設值、再慢慢進行調整)
		1.LASSO，Ridge: α 的大小
		2.決策樹：樹的深度、節點最小樣本數
		3.隨機森林：樹的數量
	2.超參數調整方法
		1.網格搜尋
			1.直接指定超參數的組合範圍，每一組參數都訓練
		2.隨機搜尋
			2.指定超參數的範圍，用均勻分布進行參數抽樣，用抽到的參數進行訓練
	3.調整步驟
		1.先將資料切分為訓練/測試集，測試集保留不使用
		2.將剛切分好的訓練集，再使用 Cross-validation 切分 K 份訓練/驗證集
		3.用 grid/random search 的超參數進行訓練與評估
		4.選出最佳的參數，用該參數與全部訓練集建模
		5.最後使用測試集評估結果
5. 集成
	1.資料面 (使用不同訓練資料 + 同一種模型)
		1.Bagging
			1.裝袋法顧名思義，是將資料放入袋中抽取，每回合結束後全部放回袋中重抽
			2.隨機森林
		2.Boosting
			1.由之前模型的預測結果，去改變資料被抽到的權重或目標值
			2.梯度提升機
	2. 模型面 (使用同一資料 + 不同模型，合成出不同預測結果)
			1.Blending (容易使用且有效)
				比如 : 線性權重0.5 + 隨機森林0.2 + 梯度提升 0.75 -> 合成預測
			2.Stacking (用blending即可)
				1.Stacking 主要是把模型當作下一階的特徵編碼器來使用
